Gibbs sampling with latent variables



## Example: Latent Dirichlet Allocation

A document is generated by these steps:

1. We have a prior distribution on topics, called Dirichlet distribution $Diri(\alpha)$. This links the document with topics. It is a distribution of topic distribution, namely prior distribution.  
2. We draw a sample from this distribution –> we draw a topic percentage from this distribution.
3. After getting the topic distribution, we can draw from another Dirichlet distribution that links the topic with words. Again, 

Remark: The key point here is the topic percentage is not fixed. Instead, we can only determine what’s the topic’s distribution. We have a prior belief in the distribution of the topic.

One can think of Dirichlet distribution as ***a distribution of distributions.***

This is different from the frequentist’s model of text classification. They say that the distribution of the topic is fixed, and we sample from this fixed distribution with known probability. Also, the distribution of words is also fixed. After determining the topic, we can sample from this topic the word of the document. 

<img src="/Users/shaoyutong/Library/Application Support/typora-user-images/Screen Shot 2023-03-02 at 9.51.50 PM.png" alt="Screen Shot 2023-03-02 at 9.51.50 PM" style="zoom:67%;" />

![Screen Shot 2023-03-02 at 9.57.08 PM](/Users/shaoyutong/Library/Application Support/typora-user-images/Screen Shot 2023-03-02 at 9.57.08 PM.png)



![Screen Shot 2023-03-02 at 10.04.18 PM](/Users/shaoyutong/Library/Application Support/typora-user-images/Screen Shot 2023-03-02 at 10.04.18 PM.png)





## Multi-stage Gibbs sampling

















## Mixture models and hierarchical models
