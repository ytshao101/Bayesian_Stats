---
title: "Module 7: Introduction to Gibbs Sampling"
author: "Yutong Shao"
date: "2023-03-06"
output: 
  pdf_document:
    extra_dependencies: ["xcolor"]
font-size: 8px
editor_options: 
  markdown: 
    wrap: 72
---

# Intoduction

In this lab, we will derive conditional distributions, code a Gibbs
sampler, and analyze the output of the Gibbs sampler.

Consider the following Exponential model for observation(s)
$x_{1:n}=(x_1,\ldots,x_n).$\footnote{Please note that in the attached data there are 30 observations, which can be found in \text{data-exponential.csv}.}:
$$ p(x|a,b) = a b \exp(- a b x) \mathbf1_{x>0},$$ where the $x_i$ are
assumed to be iid for $i=1,\ldots n.$ and suppose the prior is
$$ p(a,b) = \exp(- a - b)\mathbf1_{a,b>0}. $$ We want to sample from the
posterior $p(a,b|x_{1:n})$.

Tasks

1.  Find the conditional distributions needed for implementing a Gibbs
    sampler.
2.  Code up our own Gibbs sampler using part (1).
3.  Run the Gibbs sampler, providing convergence diagnostics.
4.  Plot a histogram or a density estimate of the estimated posterior
    using (2) and (3).
5.  How do you know that your estimated posterior in (3) is reliable?

# Task 1: Conditional distributions

Consider the following Exponential model for observation(s)
$x_{1:n}=(x_1,\ldots,x_n)$:
$$ p(x|a,b) = a b \exp(- a b x) \mathbf1_{x>0}$$ and suppose the prior
is $$ p(a,b) = \exp(- a - b)\mathbf1_{a,b>0}. $$ Our final goal is to
sample from the posterior $p(a,b|x)$.

\begin{align*}
p(\boldsymbol{x}|a,b) &= \prod_{i=1}^n p(x_i|a,b) \\
&= \prod_{i=1}^n ab\exp(-abx_i) \\
&= (ab)^n\exp\left(-ab\sum_{i=1}^nx_i\right).
\end{align*} The function is symmetric for $a$ and $b$, so we only need
to derive $p(a|\boldsymbol{x},b)$.\
This conditional distribution satisfies \begin{align*}
p(a|\boldsymbol{x},b) &\propto_a p(a,b,\boldsymbol{x}) \\
&= p(\boldsymbol{x}|a,b)p(a,b) \\
&= (ab)^n\exp\left(-ab\sum_{i=1}^nx_i \right) \times \exp(- a - b)I(a,b>0) \\
&\underset{a}{\propto} \textcolor{red}{a^{n}} \exp(-a b n\bar{x} - a)\mathbf1_{a>0} \\
&= \textcolor{red}{a^{n+1-1}} \exp(-(b n\bar{x} + 1)a)\mathbf{1}_{a>0} \\
&\underset{a}{\propto} \mathrm{Ga}(a| n+1,\, b n\bar{x} + 1).
\end{align*}

Therefore, $p(a|b,x) = \mathrm{Ga}(a| n+1,\,b n\bar{x}+1)$ and by
symmetry, $p(b|a,x) = \mathrm{Ga}(b| n+1,\,a n\bar{x}+1)$.

We now load the packages we need and the observed data $X$.

```{r}
library(MASS)
library(coda)
library(ggplot2)
X=c(
0.0371331907051745,0.271390503356533,2.40473359881563,
0.279129389456793,3.89335288168723,6.07299593757399,
2.0008345204379,0.374140078068601,0.269576448538397,
1.316028759158,2.42209106222679,1.12711988643488,
0.847399458693192,0.0916006407183253,3.15166571883227,
0.122355493965856,1.6631530013599,2.31930895969008,
0.254301124443481,2.91553389072551,1.99536939381821,
2.46937213527566,9.4394789881428,5.27195433646316,
0.45310761063482,1.65998358370856,2.95869986327957,
2.82643679251663,0.407467252515871,1.07017463071046
)
```

# Task 2: Gibbs sampling code

Here the pseudo-code

```{r}
#######################################
# This function is a Gibbs sampler
# 
# Input
#   a0: initial value for a
#   b0: initial value for b
#   S:  number of iterations
#   X:  observed data, just a vector
#
# Output
#   A two column matrix AB with samples 
     #   for a in first column and
     #   for b in second column
#######################################
```

And here the code

```{r}
sampleGibbs=function(a0, b0, S, X){
  # get sum, which is sufficient statistic
  x=sum(X)
  # get n
  n=length(X)
  # create empty matrix, allocate memory for efficiency
  AB=matrix(NA, nrow=S, ncol=2)
  # iterate the algorithm
  a=a0;b=b0
  for(j in 1:S){ # number of iteration
    AB[j,1]=a
    AB[j,2]=b
    a=rgamma(1,shape=n+1, rate=b*x+1)
    b=rgamma(1,shape=n+1, rate=a*x+1)
  }
  return(AB)
}
```

# Task 3: Run the Gibbs sampler

We start with some arbitrary initial values $a_0=b_0=1$.

```{r}
# run Gibbs sampler
a0=b0=1 # initial values = 1
S=10000
AB=sampleGibbs(a0 , b0 , S , X) 
head(AB)

df=data.frame(s=1:S, a=AB[,1], b=AB[,2])
```

We now want to assess the convergence of the algorithm, so we use a
traceplot

```{r}
ggplot(df)+
  geom_line(aes(x=s, y=a))+
  theme_bw()
ggplot(df)+
  geom_line(aes(x=s, y=b))+
  theme_bw()
```

Let's also look at the Effective Sample Size ratio computing using the
package \text{coda}.

```{r}
effectiveSize(AB[,1])/S*100
effectiveSize(AB[,2])/S*100
```

This is quite low, and indeed, even from the traceplots we can see some
**autocorrelation** (that is some local trend in the traces).
[***Remark:*** This is because the samples are too close to one another
and are very likely to be correlated.]\

A way to increase the ESS ratio, is trough \textbf{thinning}. That is
the process of **considering only samples at a certain distance from
each other**

```{r}
# AB is the original dataframe
# ABthinned is the thinned version
ABthinned=AB[seq(1,S,by=25),]
Sthinned=dim(ABthinned)[1]

print(Sthinned)

effectiveSize(ABthinned[,1])/Sthinned*100
effectiveSize(ABthinned[,2])/Sthinned*100
```

In this case we have increased the informativity of our sample but we
have also reduced its size. In a sense we have *compressed our sample*,
because the ESS of both the original samples nd the thinned version is
similar even though we have a tenfold reduction in size.

```{r}
c(effectiveSize(ABthinned[,1]),effectiveSize(AB[,1]))
c(effectiveSize(ABthinned[,2]),effectiveSize(AB[,2]))
```

Let's plot the traces of the thinned sample

```{r}
ggplot()+
  geom_line(aes(x=1:Sthinned, y=ABthinned[,1]))+
  theme_bw()
ggplot()+
  geom_line(aes(x=1:Sthinned, y=ABthinned[,2]))+
  theme_bw()
```

These plots seems to indicate convergence to a stationary distribution.
And though thinning we have manage to eliminate most of the correlation.

# Task 4

Plot a histogram or a density estimate of the estimated posterior using
tasks (2) and (3).

```{r}
prod=mean(ABthinned[,1]*ABthinned[,2]) # mean of a*b
f=function(x){return(prod/x)} 
dft=data.frame(a=ABthinned[,1],b=ABthinned[,2])
```

```{r}
ggplot(dft, aes(x = a, y = b)) +
  #geom_density_2d(aes(color = ..level..),bins=10)+
  geom_point(alpha=0.5,size=0.5)+
  xlim(0,5)+ylim(0,4)+
  geom_function(fun=f,color="red")+
  theme_bw()
```

Plot the distribution density of $a$ and $b$ (higher density when a and
b are similar and lower density when a and b are far from each other)

```{r}
ggplot(dft, aes(x = a, y = b)) +
  geom_density_2d(aes(color = ..level..),bins=10)+
  #geom_point(alpha=0.1,size=0.5)+
  xlim(0,5)+ylim(0,4)+
  geom_function(fun=f,color="red")+
  theme_bw()
```

```{r}
ggplot(dft, aes(x = a, y = b)) +
  geom_density_2d_filled() +
  scale_fill_brewer()+
  xlim(0,5)+ylim(0,4)+
  geom_function(fun=f,color="red")
```

# Task 5

Why am I plotting this thin red line?

```{r}
# the distribution of a*b
ab=ABthinned[,1]*ABthinned[,2]
ggplot()+
  geom_density(aes(x=ab))+
  theme_bw()
```

```{r}
ggplot()+
  geom_line(aes(x=1:Sthinned, y=ab))+
  theme_bw()
```

From these plots it seems that $ab$ can be estimated much more easily
than $a$ and $b$. Why is that? (\textbf{Identifiability})

**Q: How do you know that your estimated posterior in task (3) is
reliable?**

A: The density of $a$ and $b$ are very similar, so they have symmetric
posterior density, which verifies our assumption before.
